# Архитектура проекта

## Обзор

Проект реализует аналитический модуль "Карта дня" для группировки новостей в темы с помощью машинного обучения.

## Модульная структура

### 1. Конфигурация (`src/config/`)
- **settings.py**: Загрузка конфигурации из `.env` и `config.yaml`
- Поддержка переменных окружения через `${VAR_NAME}` в YAML
- Централизованное управление настройками

### 2. База данных (`src/db/`)
- **database.py**: Подключение к PostgreSQL
- Использует `psycopg2` для синхронного подключения
- Поддержка контекстного менеджера (`with`)
- Методы для получения новостей за период

### 3. Получение данных (`src/fetcher/`)
- **news_fetcher.py**: Получение новостей из БД
- Фильтрация по временному окну
- Поддержка использования только заголовков или заголовков + описания

### 4. Предобработка (`src/preprocessor/`)
- **text_cleaner.py**: Очистка и токенизация текста
- Удаление URL, email, специальных символов
- Фильтрация стоп-слов (русский язык)
- Настраиваемые параметры длины слов

### 5. Анализ (`src/analyzer/`)
- **vectorizer.py**: TF-IDF векторизация текста
- Поддержка униграмм и биграмм
- Настраиваемые параметры (max_features, min_df, max_df)
- **cluster.py**: Кластеризация через HDBSCAN
- Метрика косинусного расстояния
- Настраиваемые параметры кластеризации

### 6. Построение нарративов (`src/narrative/`)
- **builder.py**: Извлечение ключевых слов из кластеров
- Построение репрезентативных заголовков
- Сортировка кластеров по размеру
- Выбор топ-N тем

### 7. Отчеты (`src/reporter/`)
- **formatter.py**: Сохранение отчетов в JSON
- Структурированный формат с метаданными
- **summary.py**: Генерация текстового резюме
- Человекочитаемый формат для консоли

### 8. Утилиты (`src/utils/`)
- **logger.py**: Настройка логирования
- Поддержка консольного и файлового вывода
- **helpers.py**: Вспомогательные функции
- Создание директорий, форматирование дат

## Поток данных

```
1. Загрузка конфигурации
   ↓
2. Подключение к БД
   ↓
3. Получение новостей за период
   ↓
4. Предобработка текста
   ↓
5. Векторизация (TF-IDF)
   ↓
6. Кластеризация (HDBSCAN)
   ↓
7. Построение нарративов
   ↓
8. Генерация отчета (JSON + текст)
   ↓
9. Сохранение результатов
```

## Алгоритм кластеризации

1. **Предобработка**: Очистка текста, удаление стоп-слов
2. **Векторизация**: TF-IDF с униграммами и биграммами
3. **Кластеризация**: HDBSCAN с косинусной метрикой
4. **Фильтрация**: Отбор кластеров размером >= min_cluster_size
5. **Извлечение ключевых слов**: TF-IDF внутри каждого кластера
6. **Сортировка**: По размеру кластера (убывание)
7. **Топ-N**: Выбор первых N кластеров как основных тем

## Формат отчета

```json
{
  "analysis_date": "2024-01-15T00:00:00",
  "total_news": 150,
  "narratives_count": 5,
  "narratives": [
    {
      "cluster_id": 0,
      "size": 25,
      "keywords": ["война", "конфликт", "мирные переговоры"],
      "titles": ["Заголовок 1", "Заголовок 2", ...],
      "news_count": 25
    },
    ...
  ]
}
```

## Расширяемость

Проект спроектирован для легкого расширения:

1. **Добавление новых методов предобработки**: Расширить `TextCleaner`
2. **Другие алгоритмы кластеризации**: Реализовать интерфейс в `analyzer/`
3. **Альтернативные векторизации**: Word2Vec, BERT и т.д.
4. **API**: Добавить FastAPI слой поверх существующих модулей
5. **Визуализация**: Добавить модуль для генерации графиков
6. **Сохранение в БД**: Расширить `reporter` для сохранения в таблицу

## Зависимости

- **pandas, numpy**: Обработка данных
- **scikit-learn**: TF-IDF векторизация
- **hdbscan**: Кластеризация
- **nltk**: Токенизация и стоп-слова
- **psycopg2-binary**: Подключение к PostgreSQL
- **python-dotenv, pyyaml**: Конфигурация

## Производительность

- **Векторизация**: O(n * m), где n - количество новостей, m - max_features
- **Кластеризация**: O(n²) в худшем случае для HDBSCAN
- **Рекомендации**:
  - Использовать только заголовки для больших объемов данных
  - Уменьшить max_features при ограниченных ресурсах
  - Настроить min_cluster_size в зависимости от объема данных
